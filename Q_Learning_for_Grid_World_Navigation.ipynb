{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOd9EZ5dsa+pxpKkbC3+vOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sayandeep27/Reinforcement-Learning/blob/main/Q_Learning_for_Grid_World_Navigation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZXoAQC57tdQ-"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the grid world environment\n",
        "class GridWorld:\n",
        "    def __init__(self, size=(5, 5), start=(0, 0), goal=(4, 4), obstacles=[]):\n",
        "        self.size = size\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.obstacles = obstacles\n",
        "        self.state = start\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the agent's position to the starting position.\"\"\"\n",
        "        self.state = self.start\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Take a step in the environment based on the action chosen by the agent.\n",
        "\n",
        "        Args:\n",
        "        - action: The action chosen by the agent ('up', 'down', 'left', 'right').\n",
        "\n",
        "        Returns:\n",
        "        - state: The new state after taking the action.\n",
        "        - reward: The reward received for the action.\n",
        "        - done: Boolean indicating if the episode is done (reached the goal or obstacle).\n",
        "        \"\"\"\n",
        "        x, y = self.state\n",
        "        if action == 'up':\n",
        "            new_state = (x - 1, y)\n",
        "        elif action == 'down':\n",
        "            new_state = (x + 1, y)\n",
        "        elif action == 'left':\n",
        "            new_state = (x, y - 1)\n",
        "        elif action == 'right':\n",
        "            new_state = (x, y + 1)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action\")\n",
        "\n",
        "        # Check if the new state is within bounds and not an obstacle\n",
        "        if (0 <= new_state[0] < self.size[0] and\n",
        "            0 <= new_state[1] < self.size[1] and\n",
        "            new_state not in self.obstacles):\n",
        "            self.state = new_state\n",
        "\n",
        "        # Calculate reward\n",
        "        if self.state == self.goal:\n",
        "            reward = 1\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "    def visualize(self):\n",
        "        \"\"\"Visualize the grid world environment with positions marked.\"\"\"\n",
        "        grid = np.zeros(self.size)\n",
        "        grid[self.start] = 0.5  # Start position\n",
        "        grid[self.goal] = 0.8   # Goal position\n",
        "        for obs in self.obstacles:\n",
        "            grid[obs] = -1       # Obstacles\n",
        "        grid[self.state] = 0.3   # Current position\n",
        "        return grid\n"
      ],
      "metadata": {
        "id": "HC-mgvkxuOmx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Q-learning agent\n",
        "class QLearningAgent:\n",
        "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the Q-learning agent.\n",
        "\n",
        "        Args:\n",
        "        - env: The grid world environment object.\n",
        "        - alpha: Learning rate (default: 0.1).\n",
        "        - gamma: Discount factor (default: 0.9).\n",
        "        - epsilon: Exploration-exploitation trade-off (default: 0.1).\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration-exploitation trade-off\n",
        "        # Initialize Q-table with zeros, dimensions are (rows, columns, actions)\n",
        "        self.q_table = np.zeros((env.size[0], env.size[1], 4))  # 4 actions: 'up', 'down', 'left', 'right'\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Choose an action for the current state based on epsilon-greedy policy.\n",
        "\n",
        "        Args:\n",
        "        - state: The current state of the agent.\n",
        "\n",
        "        Returns:\n",
        "        - action: The chosen action ('up', 'down', 'left', 'right').\n",
        "        \"\"\"\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            # Exploration: choose a random action\n",
        "            return np.random.choice(['up', 'down', 'left', 'right'])\n",
        "        else:\n",
        "            # Exploitation: choose action with highest Q-value for this state\n",
        "            return ['up', 'down', 'left', 'right'][np.argmax(self.q_table[state])]\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        Update the Q-table based on the agent's experience.\n",
        "\n",
        "        Args:\n",
        "        - state: The current state of the agent.\n",
        "        - action: The action taken in the current state ('up', 'down', 'left', 'right').\n",
        "        - reward: The reward received for taking the action.\n",
        "        - next_state: The resulting state after taking the action.\n",
        "        \"\"\"\n",
        "        # Retrieve the current Q-value for the state-action pair\n",
        "        old_value = self.q_table[state][['up', 'down', 'left', 'right'].index(action)]\n",
        "        # Calculate the maximum Q-value for the next state\n",
        "        next_max = np.max(self.q_table[next_state])\n",
        "        # Update the Q-value based on the Q-learning formula\n",
        "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
        "        self.q_table[state][['up', 'down', 'left', 'right'].index(action)] = new_value\n",
        "\n",
        "    def train(self, episodes=100):\n",
        "        \"\"\"\n",
        "        Train the Q-learning agent to learn the optimal policy.\n",
        "\n",
        "        Args:\n",
        "        - episodes: Number of episodes to train the agent (default: 100).\n",
        "        \"\"\"\n",
        "        for _ in range(episodes):\n",
        "            self.env.reset()  # Reset the environment for each episode\n",
        "            state = self.env.state  # Get the initial state\n",
        "            done = False  # Initialize episode completion flag\n",
        "            while not done:\n",
        "                action = self.choose_action(state)  # Choose action based on current state\n",
        "                next_state, reward, done = self.env.step(action)  # Take action and observe next state and reward\n",
        "                self.update_q_table(state, action, reward, next_state)  # Update Q-table based on experience\n",
        "                state = next_state  # Move to the next state\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"\n",
        "        Test the trained Q-learning agent by navigating from start to goal.\n",
        "\n",
        "        Returns:\n",
        "        - path: List of states representing the optimal path found by the agent.\n",
        "        \"\"\"\n",
        "        self.env.reset()  # Reset the environment\n",
        "        state = self.env.state  # Get the initial state\n",
        "        path = [state]  # Initialize path with start state\n",
        "        while state != self.env.goal:\n",
        "            action = ['up', 'down', 'left', 'right'][np.argmax(self.q_table[state])]  # Choose action with highest Q-value\n",
        "            next_state, _, _ = self.env.step(action)  # Take action and observe next state (ignore reward and done)\n",
        "            state = next_state  # Move to the next state\n",
        "            path.append(state)  # Add state to the path\n",
        "        return path"
      ],
      "metadata": {
        "id": "Fw4t5Wnhucnt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the grid world environment\n",
        "    size = (5, 5)\n",
        "    start = (0, 0)\n",
        "    goal = (4, 4)\n",
        "    obstacles = [(1, 1), (2, 2), (3, 3)]\n",
        "    env = GridWorld(size=size, start=start, goal=goal, obstacles=obstacles)\n",
        "\n",
        "    # Initialize Q-learning agent\n",
        "    agent = QLearningAgent(env)\n",
        "\n",
        "    # Train the agent\n",
        "    agent.train(episodes=100)\n",
        "\n",
        "    # Test the agent\n",
        "    path = agent.test()\n",
        "    print(\"Optimal Path Found by Agent:\", path)\n",
        "\n",
        "    # Visualize the grid world environment\n",
        "    print(\"Grid World Visualization:\")\n",
        "    print(env.visualize())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqCwYrKTuii5",
        "outputId": "481742cc-e84d-4a3c-a1cb-f36c8502a007"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Path Found by Agent: [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
            "Grid World Visualization:\n",
            "[[ 0.5  0.   0.   0.   0. ]\n",
            " [ 0.  -1.   0.   0.   0. ]\n",
            " [ 0.   0.  -1.   0.   0. ]\n",
            " [ 0.   0.   0.  -1.   0. ]\n",
            " [ 0.   0.   0.   0.   0.3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qeex9zzrunBw"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}